{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN4080: obligatory assignment 1 (Autumn 2024)\n",
    " \n",
    "Mandatory assignment 1 consists of three parts. In Part 1 (6 points), you will test and improve on a BPE (Byte-Pair-Encoding) tokenizer . In Part 2 (7 points), you will estimate an N-gram language model, based on a training corpus and the tokenizer you worked on in Part 1. Finally, in Part 3 (7 points), you will develop a basic classification model to distinguish between Bokmål and Nynorsk sentences.\n",
    "\n",
    "You should answer all three parts. You are required to get at least 12/20 points to pass. The most important is that you try to answer each question (possibly with some mistakes), to help you gain a better and more concrete understanding of the topics covered during the lectures. There are also bonus questions for those of you who would like to deepen their understanding of the topics covered by this assignment.\n",
    "\n",
    "- We assume that you have read and are familiar with IFI’s requirements and guidelines for mandatory assignments, see [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html) and [here](https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-guidelines.html).\n",
    "- This is an individual assignment. You should not deliver joint submissions. \n",
    "- You may redeliver in Devilry before the deadline (__Sunday, September 15 at 23:59__), but include all files in the last delivery.\n",
    "- Only the last delivery will be read! If you deliver more than one file, put them into a zip-archive. You don't have to include in your delivery the files already provided for this assignment. \n",
    "- Name your submission _your\\_username\\_in4080\\_mandatory\\_1_\n",
    "- You can work on this assignment either on the IFI machines or on your own computer. \n",
    "\n",
    "*The preferred format for the assignment is a completed version of this Jupyter notebook*, containing both your code and explanations about the steps you followed. We want to stress that simply submitting code is __not__ by itself sufficient to complete the assignment - we expect the notebook to also contain explanations of what you have implemented, along with motivations for the choices you made along the way. Preferably use whole sentences, and mathematical formulas if necessary. Explaining in your own words (using concepts we have covered through in the lectures) what you have done and reflecting on your solution is an important part of the learning process - take it seriously!\n",
    "\n",
    "Regarding the use of LLMs (ChatGPT or similar): you are allowed to use them as 'sparring partner', for instance to clarify something you have not understood. However, you are __not__ allowed to use them to generate solutions (either in part or in full) to the assignment tasks. \n",
    "\n",
    "__Technical tip__: Some of the tasks in this assignment will require you to extend methods in classes that are already partly implemented. To implement those methods directly in a Jupyter notebook, you can use the function `setattr` to attach a method to a given class: \n",
    "\n",
    "```python\n",
    "class A:\n",
    "    pass\n",
    "a = A()\n",
    "\n",
    "def foo(self):\n",
    "    print('hello world!')\n",
    "    \n",
    "setattr(A, 'foo', foo)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : Tokenisation\n",
    "\n",
    "----------------\n",
    "The **tokenisation** is the process of dividing a text into **tokens**, a single unit representing a part of the text. It can be:\n",
    "   - A whole word (‘cat’).\n",
    "   - A part of a word (‘gat-’ or ‘-to’).\n",
    "   - A single character (‘g’, ‘a’, ‘t’, ‘o’).\n",
    "   - A symbol or punctuation (‘,’, ‘.’, ‘!’).\n",
    "   - An abstract entity such as a whitespace or special symbols in the pattern.\n",
    "\n",
    "The goal of tokenization is to obtain an optimal representation of the text, balancing the granularity of the tokens with the ability of the model to understand their meaning. Among the different algorithms used for this process, two of the most common are:\n",
    "\n",
    "- **Byte-Pair Encoding (BPE)**: This algorithm is based on the frequency with which pairs of characters or subwords appear in the text. The process begins by dividing the text into individual characters, then iteratively combines the most frequent characters or sequences of characters to form larger tokens. This procedure continues until a predetermined number of tokens in the vocabulary is reached, thus allowing an efficient representation for both common words and rare or new terms.\n",
    "\n",
    "- **Unigram**: Unlike BPE, the Unigram algorithm starts with a large set of possible tokens and progressively eliminates them based on the statistical probability of each token to represent the text well. In this way, the algorithm retains only the tokens that are most useful for understanding the text, achieving a balance between accuracy and vocabulary size.\n",
    "\n",
    "Tokenization is a fundamental process in Natural Language Processing (NLP) models, as it transforms natural language text into a format that models can process, such as sequences of numbers or vectors. This step is crucial for enabling mathematical models to understand and work with human language. Achieving an optimal balance between the granularity of tokens and their interpretability is key to ensuring that language models perform effectively.\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by building a basic tokenizer relying on white space and punctuation. \n",
    "\n",
    "__Task 1.1__ (2 points): Implement the method `split` below such that it takes a text as input and outputs a list of tokens. The tokenisation should simply be done by splitting on white space, except for punctuation markers and other symbols (`.,:;!?-()\"`), which should correspond to their own token. For instance, the sentence \"Pierre, who works at NR, also teaches at UiO.\" should be split into 12 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', ',', 'who', 'works', 'at', 'NR', ',', 'also', 'teaches', 'at', 'UiO', '.']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "string = \"Pierre, who works at NR, also teaches at UiO.\"\n",
    "string_split = re.split(r'(\\s+|[.,:;!?()|\"/-])', string)\n",
    "string_split = [t for t in string_split if t.strip()]\n",
    "print(string_split)\n",
    "print(len(string_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def basic_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"The method should split the text on white space, except for punctuation\n",
    "    markers that should be considered as tokens of their own (even in the \n",
    "    absence of white space before or after their occurrence)\"\"\"\n",
    "\n",
    "    # Implement here your basic tokenisation\n",
    "    string_split = re.split(r'(\\s+|[.,:;!?()|\"/-])', text)\n",
    "    string_split = [t for t in string_split if t.strip()]\n",
    "    print(\"Number of tokens are:\\t\", len(string_split), \"\\nNumber of types are:\\t\", len(set(string_split)))\n",
    "    return string_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens are:\t 12 \n",
      "Number of types are:\t 10\n"
     ]
    }
   ],
   "source": [
    "y = basic_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the tokeniser on a small corpus, the [Norwegian Dependency Treebank](https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-10/) (the corpus has been annotated with morphological features, syntactic functions and hierarchical structures, but we'll simply use here the raw text and discard all the annotation layers). We provide you with the data in the files `ndt_train_lm.txt` and `ndt_test_lm.txt`. \n",
    "\n",
    "__Task 1.2__ (1 point): Run the tokenizer you have implemented on `ndt_test_lm.txt`. How many tokens were extracted? And how many types (distinct words) were there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens are:\t 259236 \n",
      "Number of types are:\t 30050\n"
     ]
    }
   ],
   "source": [
    "with open(\"ndt_test_lm.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    ndt_test_lm = file.read()\n",
    "\n",
    "tokens_types = basic_tokenize(ndt_test_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now use Byte-Pair Encoding (BPE) to limit the vocabulary of the tokenizer to 5,000.  An initial implementation of the algorithm is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Iterator\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"Tokenizer based on the Byte-Pair Encoding algorithm. \n",
    "    Note: the current implementation is limited to Latin characters (ISO-8859-1)\"\"\"\n",
    "\n",
    "    def __init__(self, train_corpus_file: str, vocab_size = 5000):\n",
    "        \"\"\"Creates a new BPE tokenizer, with merge pairs found using the given\n",
    "        corpus file. The extraction of merge pairs stops when a vocabulary of \n",
    "        size vocab_size is reached.\"\"\"\n",
    "\n",
    "        # List of string pairs that should be merged when tokenizing\n",
    "        # Example: ('e', 't'), which means that 'et' is a possible subword\n",
    "        # Each string pair is mapped to an unique index number\n",
    "        # (corresponding to their position in the self.vocab list)\n",
    "        self.merge_pairs = {}\n",
    "\n",
    "        # We add as basic vocab all characters of the extended ASCII\n",
    "        self.vocab = [chr(i) for i in range(256)]\n",
    "\n",
    "        with open(train_corpus_file) as fd:\n",
    "\n",
    "            # We first read the corpus, split on white space, and counts the\n",
    "            # occurrences of each distinct word\n",
    "            print(\"Counting word occurrences in corpus %s\"%train_corpus_file, end=\"...\", flush=True)\n",
    "            text = fd.read()\n",
    "            vocabulary_counts = {}\n",
    "            for token in text.split():\n",
    "                vocabulary_counts[token] = vocabulary_counts.get(token, 0) + 1\n",
    "            print(\"Done\")\n",
    "\n",
    "            # We then iteratively extend the list of merge pairs until we\n",
    "            # reach the desired size. Note: to speed up the algorithm, we \n",
    "            # extract n merge pairs at each iteration\n",
    "            progress_bar = tqdm(total=vocab_size)\n",
    "            while len(self.vocab) < vocab_size:\n",
    "                most_common_pairs = self.get_most_common_pairs(vocabulary_counts)\n",
    "                for common_pair in most_common_pairs:\n",
    "                    self.merge_pairs[common_pair] = len(self.vocab)\n",
    "                    self.vocab.append(\"\".join(common_pair))\n",
    "                progress_bar.update(len(most_common_pairs))\n",
    "        #       print(\"Examples of new subwords:\", [\"\".join(pair) for pair in most_common_pairs][:10])\n",
    "            \n",
    "    def get_most_common_pairs(self, vocabulary_counts: Dict[str,int], \n",
    "                              n:int=200) -> List[Tuple[str,str]]:\n",
    "        \"\"\"Given a set of distinct words along with their corresponding number \n",
    "        of occurrences in the corpus, returns the n most frequent pairs of subwords.       \n",
    "        \"\"\"\n",
    "\n",
    "        # We count the frequencies of consecutive subwords in the vocabulary list\n",
    "        pair_freqs = {}\n",
    "        for word, word_count in vocabulary_counts.items():\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for i in range(len(subwords)-1):\n",
    "                byte_pair = (subwords[i], subwords[i+1])\n",
    "                pair_freqs[byte_pair] = pair_freqs.get(byte_pair, 0) + word_count\n",
    "\n",
    "        # And return the most frequent ones\n",
    "        most_freq_pairs = sorted(pair_freqs.keys(), key=lambda x: pair_freqs[x])[::-1][:n]\n",
    "        return most_freq_pairs\n",
    "\n",
    "    def __call__(self, input:str, show_progress_bar=True) -> Iterator[str]:\n",
    "        \"\"\"Tokenizes a full text\"\"\"\n",
    "\n",
    "        # We first split into whitespace-separated tokens, and then in subwords\n",
    "        words = input.split()\n",
    "        for word in tqdm(words) if show_progress_bar else words:\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for subword in subwords:\n",
    "                yield subword\n",
    "                \n",
    "\n",
    "    def tokenize_word(self, word):\n",
    "        \"\"\"Splits the word into subwords, according to the merge pairs \n",
    "        currently stored in self.merge_pairs.\"\"\"\n",
    "\n",
    "        # We start with a list of characters\n",
    "        # (+ a final character to denote the end of the word)    \n",
    "        splits = list(word) + [\" \"]\n",
    "\n",
    "        # We continue until there is nothing left to be merged\n",
    "        while len(splits)>=2:\n",
    "\n",
    "            # We extract consecutive subword pairs\n",
    "            pairs = [(splits[i], splits[i+1]) for i in range(len(splits)-1)]\n",
    "\n",
    "            # We find the \"best\" pair of subwords to merge -- that is, the one with the \n",
    "            # lowest position in the list of merge rules\n",
    "            best_pair_to_merge = min(pairs, key=lambda x: self.merge_pairs.get(x, np.inf))\n",
    "            if best_pair_to_merge in self.merge_pairs:\n",
    "\n",
    "                # We then merge the two subwords\n",
    "                for i in range(len(splits)-1):\n",
    "                    if (splits[i], splits[i+1]) == best_pair_to_merge:\n",
    "                        merged_subword = self.vocab[self.merge_pairs[best_pair_to_merge]]\n",
    "                        splits = splits[:i] + [merged_subword] + splits[i+2:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.3__ (1 point): Learn the BPE tokenizer on the `ndt_train_lm.txt` corpus, and then apply this tokenizer on `ndt_test_lm.txt`. Print the number of tokens and types (distinct subwords) obtained by this tokenizer on the test data. How do those numbers compare to the ones obtained with the basic tokenizer you had implemented earlier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ndt_train_lm.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    ndt_train_lm = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word occurrences in corpus ndt_train_lm.txt...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191c9671cd244d8e89f42b14424b6620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6db67f890614fada0beade60ba19326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ndt_training_lm = BPETokenizer(train_corpus_file = 'ndt_train_lm.txt', vocab_size = 5000)\n",
    "\n",
    "ndt_testing_lm = list(ndt_training_lm(ndt_test_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens are:\t 386099\n",
      "Number of types are:\t 4408\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens are:\\t\", len(ndt_testing_lm))\n",
    "print(\"Number of types are:\\t\", len(set(ndt_testing_lm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do those numbers compare to the ones obtained with the basic tokenizer you had implemented earlier ?**\n",
    "|  | basic_tokenize | BPETokenizer |\n",
    "|-----------|-----------|-----------|\n",
    "| Number of tokens | 259236  | 386099 |\n",
    "| Number of types | 30050 | 4408 |\n",
    "\n",
    "\n",
    "The number of tokens and type are different because of the byte pair coding, because I just implement a tokenization at character level, so I only split words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab dim:\t 5056\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab dim:\\t\", len(ndt_training_lm.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.4__ (2 points): The current BPE implementation is that it treats all characters in the same manner. A rather inconvenient side effect is that letters may be merged together with punctuation markers (like 'ing', ',' --> 'ing,'), if they are not separated by white space. Modify the implementation of the BPE algorithm above to prevent punctuation markers to be merged with letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class BPETokenizer2:\n",
    "    \"\"\"Tokenizer based on the Byte-Pair Encoding algorithm, extended to handle punctuation splitting.\n",
    "    Note: the current implementation handles Latin characters (ISO-8859-1) and punctuation.\"\"\"\n",
    "\n",
    "    def __init__(self, train_corpus_file: str, vocab_size=5000):\n",
    "        \"\"\"Creates a new BPE tokenizer, with merge pairs found using the given\n",
    "        corpus file. The extraction of merge pairs stops when a vocabulary of \n",
    "        size vocab_size is reached.\"\"\"\n",
    "        self.merge_pairs = {}\n",
    "        self.vocab = [chr(i) for i in range(256)] + list('.,!?;:\"\\'()[]{}')\n",
    "\n",
    "        with open(train_corpus_file) as fd:\n",
    "            print(\"Counting word occurrences in corpus %s\" % train_corpus_file, end=\"...\", flush=True)\n",
    "            text = fd.read()\n",
    "            vocabulary_counts = {}\n",
    "            for token in self.split_on_punctuation(text):\n",
    "                vocabulary_counts[token] = vocabulary_counts.get(token, 0) + 1\n",
    "            print(\"Done\")\n",
    "\n",
    "            progress_bar = tqdm(total=vocab_size)\n",
    "            while len(self.vocab) < vocab_size:\n",
    "                most_common_pairs = self.get_most_common_pairs(vocabulary_counts)\n",
    "                for common_pair in most_common_pairs:\n",
    "                    self.merge_pairs[common_pair] = len(self.vocab)\n",
    "                    self.vocab.append(\"\".join(common_pair))\n",
    "                progress_bar.update(len(most_common_pairs))\n",
    "\n",
    "    def split_on_punctuation(self, text: str) -> List[str]:\n",
    "        \"\"\"Splits text into words and punctuation marks\"\"\"\n",
    "        return re.findall(r\"\\w+|[.,!?;:]\", text)\n",
    "\n",
    "    def get_most_common_pairs(self, vocabulary_counts: Dict[str, int], n: int = 200) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Returns the n most frequent pairs of subwords.\"\"\"\n",
    "        pair_freqs = {}\n",
    "        for word, word_count in vocabulary_counts.items():\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for i in range(len(subwords) - 1):\n",
    "                byte_pair = (subwords[i], subwords[i + 1])\n",
    "                pair_freqs[byte_pair] = pair_freqs.get(byte_pair, 0) + word_count\n",
    "        most_freq_pairs = sorted(pair_freqs.keys(), key=lambda x: pair_freqs[x])[::-1][:n]\n",
    "        return most_freq_pairs\n",
    "\n",
    "    def __call__(self, input: str, show_progress_bar=True) -> Iterator[str]:\n",
    "        \"\"\"Tokenizes a full text.\"\"\"\n",
    "        words = self.split_on_punctuation(input)\n",
    "        for word in tqdm(words) if show_progress_bar else words:\n",
    "            subwords = self.tokenize_word(word)\n",
    "            for subword in subwords:\n",
    "                yield subword\n",
    "\n",
    "    def tokenize_word(self, word: str) -> List[str]:\n",
    "        \"\"\"Splits the word into subwords, according to the merge pairs.\"\"\"\n",
    "        splits = list(word) + [\" \"]\n",
    "        while len(splits) >= 2:\n",
    "            pairs = [(splits[i], splits[i + 1]) for i in range(len(splits) - 1)]\n",
    "            best_pair_to_merge = min(pairs, key=lambda x: self.merge_pairs.get(x, np.inf))\n",
    "            if best_pair_to_merge in self.merge_pairs:\n",
    "                for i in range(len(splits) - 1):\n",
    "                    if (splits[i], splits[i + 1]) == best_pair_to_merge:\n",
    "                        merged_subword = self.vocab[self.merge_pairs[best_pair_to_merge]]\n",
    "                        splits = splits[:i] + [merged_subword] + splits[i + 2:]\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word occurrences in corpus ndt_train_lm.txt...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75570c80decc486daaa7ea872542dd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c208107986b412b8c27fe1f796a2b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ndt_training_lm2 = BPETokenizer2(train_corpus_file = 'ndt_train_lm.txt', vocab_size = 5000)\n",
    "\n",
    "ndt_testing_lm2 = list(ndt_training_lm2(ndt_test_lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens are:\t 383605\n",
      "Number of types are:\t 4306\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens are:\\t\", len(ndt_testing_lm2))\n",
    "print(\"Number of types are:\\t\", len(set(ndt_testing_lm2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab dim:\t 5070\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab dim:\\t\", len(ndt_training_lm2.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.5__ (_optional, 2 extra points_): In a [tweet](https://x.com/karpathy/status/1759996551378940395) published earlier this year, the well-known AI researcher Andrej Karpathy stressed that many of the current limitations of Large Language Models are actually a product of the tokenisation step. Explain at least 4 of the problems he mentioned in his tweet (you can of course search online, or watch Karpathy's own video lecture on tokenization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Why can't LLM do super simple string processing tasks like reversing a string?**\n",
    "Tokenisation affects the way LLM sees text. Since text is divided into tokens rather than characters, inverting a string, which seems simple for humans, can become complex for an LLM working with tokenised units.\n",
    "\n",
    "2. **Why is LLM bad at simple arithmetic?**\n",
    "Simple arithmetic can be problematic because numbers might be tokenized in a way that doesn’t preserve their numerical meaning. For example, breaking down a number like \"12345\" into multiple tokens can confuse the model during mathematical operations.\n",
    "\n",
    "3. **Why did GPT-2 have more than necessary trouble coding in Python?** \n",
    "In programming languages, even small changes in tokens (like removing a whitespace or breaking a string into unexpected parts) can lead to significant errors. This also applies to GPT models like GPT-2 that tokenize code ineffectively, causing trouble in generating accurate or functional code.\n",
    "\n",
    "4. **What is this weird warning I get about a \"trailing whitespace\"?**\n",
    "Tokenization can treat whitespace characters as significant, so trailing whitespace might generate errors or warnings. This is particularly problematic in environments like code generation, where whitespace can be crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: N-gram Language Models\n",
    "\n",
    "-----\n",
    "An **N-gram** language model is a probabilistic model used to predict the next word in a sequence of text based on the previous $N-1$ words. N-grams are sequences of $N$ consecutive elements (usually words or characters) that appear in a text.\n",
    "\n",
    "The main goal of an N-gram language model is to estimate the conditional probability of a word given a sequence of preceding words. Mathematically, the problem of modeling a sequence of words $w_1, w_2, ..., w_n$ is based on calculating the joint probability:\n",
    "\n",
    "$$\n",
    "P(w_1, ..., w_n) = \\prod_{k=1}^n P(w_k \\mid w_1, w_2, ..., w_{k-1})\n",
    "$$\n",
    "\n",
    "However, this approach becomes computationally impractical for long sequences, as the amount of data required to accurately estimate these probabilities grows exponentially with the length of the sequence.\n",
    "\n",
    "### The Markov assumption\n",
    "To solve this problem, a **Markov assumption** is used: it is assumed that the probability of a word depends only on the previous $N-1$ words. In other words, instead of considering the entire preceding sequence of words, only the last $N-1$ words are taken into account. The N-gram model thus approximates the probability of a word $w_i$ as:\n",
    "\n",
    "$$\n",
    "P(w_i | w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-N+1}, ..., w_{i-1})\n",
    "$$\n",
    "\n",
    "Depending on the value of $N$, the model has different names:\n",
    "- **Unigram** ($N=1$): The probability of each word is independent of the others. The probability of a sequence is simply the product of the probabilities of each word:\n",
    "  \n",
    "  $$\n",
    "  P(w_i \\mid w_1, ..., w_{i-1}) \\approx P(w_i)\n",
    "  $$\n",
    "\n",
    "- **Bigram** ($N=2$): The probability of each word depends only on the previous word:\n",
    "\n",
    "  $$\n",
    "  P(w_i \\mid w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-1})\n",
    "  $$\n",
    "\n",
    "- **Trigram** ($N=3$): The probability of each word depends on the two previous words:\n",
    "\n",
    "  $$\n",
    "  P(w_i \\mid w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-2}, w_{i-1})\n",
    "  $$\n",
    "\n",
    "### Probability Estimation\n",
    "To calculate probabilities in an N-gram model, the frequencies of N-grams observed in the training corpus are used. The conditional probability of an N-gram can be estimated as:\n",
    "\n",
    "$$\n",
    "P(w_i | w_{i-N+1}, ..., w_{i-1}) = \\frac{C(w_{i-N+1}, ..., w_{i-1}, w_i)}{C(w_{i-N+1}, ..., w_{i-1})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $C(w_{i-N+1}, ..., w_{i-1}, w_i)$ is the number of times the sequence of N words appears in the corpus.\n",
    "- $C(w_{i-N+1}, ..., w_{i-1})$ is the number of times the first $N-1$ words of the sequence appear in the corpus.\n",
    "\n",
    "### Smoothing\n",
    "One of the main problems with N-gram models is **data sparsity**: many possible N-grams may never appear or appear very rarely in the training corpus, making it difficult to estimate their probabilities. To address this issue, **smoothing** techniques are used, such as:\n",
    "\n",
    "- **Laplace Smoothing**: Adds one to each count to avoid zero probabilities:\n",
    "\n",
    "  $$\n",
    "  P(w_i | w_{i-N+1}, ..., w_{i-1}) = \\frac{C(w_{i-N+1}, ..., w_{i-1}, w_i) \\textcolor{cyan}{+ 1}}{C(w_{i-N+1}, ..., w_{i-1}) \\textcolor{cyan}{+ 1 \\cdot V}}\n",
    "  $$\n",
    "\n",
    "  where $V$ is the total number of tokens in the vocabulary.\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train simple N-gram language models on the NDT corpus, using the tokenizers we have developed in Part 1.\n",
    "\n",
    "Here is the skeleton of the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import abstractmethod\n",
    "\n",
    "class LanguageModel:\n",
    "    \"\"\"Generic class for running operations on language models, using a BPE tokenizer\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: BPETokenizer):\n",
    "        \"\"\"Build an abstract language model using the provided tokenizer\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, context_tokens: List[str]):\n",
    "        \"\"\"Given a list of context tokens (=previous tokens), returns a dictionary\n",
    "           mapping each possible token to its probability\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_perplexity(self, text: str):\n",
    "        \"\"\"Computes the perplexity of the given text according to the LM\"\"\"\n",
    "\n",
    "        print(\"Tokenising input text:\")\n",
    "        tokens = list(self.tokenizer(text))\n",
    "        \n",
    "        print(\"Computing perplexity:\")\n",
    "        log_probs = 0\n",
    "        for i in tqdm(range(len(tokens))):\n",
    "            context_tokens = [\"<s>\"] + tokens[:i]\n",
    "            predict_distrib = self.predict(context_tokens)\n",
    "\n",
    "            # We add the log-probabilities\n",
    "            log_probs += np.log(predict_distrib[tokens[i]])\n",
    "            \n",
    "        perplexity = np.exp(-log_probs/len(tokens))\n",
    "        return perplexity\n",
    "\n",
    "class NGramLanguageModel(LanguageModel):\n",
    "    \"\"\"Representation of a N-gram-based language model\"\"\"\n",
    "\n",
    "    def __init__(self, training_corpus_file: str, tokenizer:BPETokenizer, ngram_size:int=3,\n",
    "                 alpha_smoothing:float=1):\n",
    "        \"\"\"Initialize the N-gram model with:\n",
    "        - a file path to a training corpus to estimate the N-gram probabilities\n",
    "        - an already learned BPE tokenizer\n",
    "        - an N-gram size\n",
    "        - a smoothing parameter (Laplace smoothing)\"\"\"\n",
    "        \n",
    "        LanguageModel.__init__(self, tokenizer)\n",
    "        self.ngram_size = ngram_size\n",
    "        \n",
    "        # We define a simple backoff distribution (here just a uniform distribution)\n",
    "        self.default_distrib = {token:1/len(tokenizer.vocab) for token in tokenizer.vocab}\n",
    "\n",
    "        # Dictionary mapping a context (for instance the two preceding words if ngram_size=3)\n",
    "        # to another dictionary specifying the probability of each possible word in the \n",
    "        # vocabulary. The context should be a tuple of tokens.\n",
    "        self.ngram_probs = {}\n",
    "        with open(training_corpus_file) as fd:   \n",
    "\n",
    "            # based on the training corpus, tokenizer, ngram-size and smoothing parameter,\n",
    "            # fill the self.ngram_probs with the correct N-gram probabilities  \n",
    "            raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def predict(self, context_tokens: List[str]):\n",
    "        \"\"\"Given a list of preceding tokens, returns the probability distribution \n",
    "        over the next possible token.\"\"\"\n",
    "\n",
    "        # We restrict the contextual tokens to (N-1) tokens\n",
    "        context_tokens = tuple(context_tokens[-self.ngram_size+1:])\n",
    "\n",
    "        # If the contextual tokens were indeed observed in the corpus, simply\n",
    "        # returns the precomputed probabilities\n",
    "        if context_tokens in self.ngram_probs:\n",
    "            return self.ngram_probs[context_tokens]\n",
    "        \n",
    "        # Otherwise, we return a uniform distribution over possible tokens\n",
    "        else:\n",
    "            return self.default_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.1__ (6 points): Complete the initialization method `__init__` to estimate the correct N-gram probabilities (with smoothing) based on the corpus. Don't worry about making your implementation super-efficient (although you can if you wish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def __init__(self, training_corpus_file: str, tokenizer: BPETokenizer, ngram_size: int = 2, alpha_smoothing: float = 0.1):\n",
    "    \"\"\"Initialize the N-gram model with:\n",
    "    - a file path to a training corpus to estimate the N-gram probabilities\n",
    "    - an already learned BPE tokenizer\n",
    "    - an N-gram size\n",
    "    - a smoothing parameter (Laplace smoothing)\n",
    "    \"\"\"\n",
    "    LanguageModel.__init__(self, tokenizer)\n",
    "    self.ngram_size = ngram_size\n",
    "\n",
    "    # Define a simple backoff distribution (uniform distribution)\n",
    "    self.default_distrib = {token: 1/len(tokenizer.vocab) for token in tokenizer.vocab}\n",
    "\n",
    "    # Dictionary mapping contexts (tuples of preceding words) to probability distributions\n",
    "    self.ngram_probs = defaultdict(Counter)\n",
    "    context_counts = defaultdict(Counter)\n",
    "    \n",
    "    # Read the corpus and tokenize it\n",
    "    with open(training_corpus_file, 'r') as fd:\n",
    "        for line in fd:\n",
    "            tokens = [\"<s>\"] + list(self.tokenizer(line.strip())) + [\"</s>\"]\n",
    "            \n",
    "            # Create n-grams and context for each token\n",
    "            for i in range(len(tokens) - ngram_size + 1):\n",
    "                context = tuple(tokens[i:i + ngram_size - 1])\n",
    "                token = tokens[i + ngram_size - 1]\n",
    "                context_counts[context][token] += 1\n",
    "    \n",
    "    # Calculate smoothed probabilities\n",
    "    vocab_size = len(self.tokenizer.vocab)\n",
    "    \n",
    "    for context, token_counts in context_counts.items():\n",
    "        total_count = sum(token_counts.values()) + alpha_smoothing * vocab_size\n",
    "        for token, count in token_counts.items():\n",
    "            self.ngram_probs[context][token] = (count + alpha_smoothing) / total_count\n",
    "        \n",
    "        for token in self.tokenizer.vocab:\n",
    "            if token not in self.ngram_probs[context]:\n",
    "                self.ngram_probs[context][token] = alpha_smoothing / total_count\n",
    "\n",
    "setattr(NGramLanguageModel, '__init__', __init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "### Evaluation of Language Models (Perplexity)\n",
    "\n",
    "**Perplexity** is a metric used to evaluate language models, representing how well a model predicts a sequence of words. It is defined as the inverse probability of a sentence, normalized by the number of words:\n",
    "\n",
    "$$\n",
    "PPL(w_1, \\dots, w_n) = \\sqrt[n]{\\frac{1}{P(w_1, \\dots, w_n)}}\n",
    "$$\n",
    "\n",
    "- Minimizing perplexity is equivalent to maximizing the probability of the word sequence.\n",
    "- The probability of a sequence lies within the range $[0, 1]$, while perplexity ranges from $1$ to $\\infty$.\n",
    "\n",
    "When applying the **bigram Markov assumption**, perplexity is calculated as follows:\n",
    "\n",
    "$$\n",
    "PPL(w_1, \\dots, w_n) = \\sqrt[n]{\\prod_{i=1}^{n} \\frac{1}{P(w_i \\mid w_{i-1})}}\n",
    "$$\n",
    "\n",
    "In this case, the model assumes that the probability of each word depends only on the previous word in the sequence.\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.2__ (1 point): Train your language model in `ndt_train_lm.txt`, and compute its perplexity on the test data in `ndt_test_lm.txt`. The perplexity can be computed by calling the method `get_perplexity`. <br>\n",
    "(_Note_: if the training takes too much time, feel free to stop the process after looking at a fraction of the corpus, at least while you are testing/developing your training setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word occurrences in corpus ndt_train_lm.txt...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303200009a6142f1b31fc6879d8bae7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4104ab296e49209856b6e00bc1c296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/872824 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = NGramLanguageModel(training_corpus_file='ndt_train_lm.txt', tokenizer=BPETokenizer(train_corpus_file= 'ndt_train_lm.txt', vocab_size = 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenising input text:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0279f828153746a48437170bbbc41b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/226096 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing perplexity:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58935527e34142d1990ca1d122c30975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/386099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 193.0345419254361\n"
     ]
    }
   ],
   "source": [
    "perplexity = model.get_perplexity(ndt_test_lm)\n",
    "print(f\"Test Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Perplexity| 193.0345419254361\n",
    "|-----------|-----------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 2.3__ (_optional_, 4 bonus points): Improve the language model you have just developed. You can choose to focus on improving your model through a backoff mechanism, interpolation, or both. Once you are done, compute the perplexity again on the test corpus to ensure the language model has indeed improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text classification\n",
    "\n",
    "We will finally use the texts from the Norwegian Dependency Treebank for a classification task -- more precisely to determine whether a sentence is likely to be written in Bokmål or Nynorsk. To this end, we will use a simple bag-of-word setup (or more precisely a bag-of-_subwords_, since we will rely on the subwords extracted using BPE) along with a logistic regression model. As there is only two, mutually exclusive classes, you can view the task as a binary classification problem. \n",
    "\n",
    "The training data is found in `ndt_train_class.txt` and simply consists of a list of sentences, each sentence being mapped to a language form (Bokmål: `nob` or Nynorsk: `nno`). The language form is written at the end of each line, separated by a `\\t`. Note the training examples are currently _not_ shuffled.\n",
    "\n",
    "To train and apply your classifier, the easiest is to use the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.1__ (2 points): Create a `N x V` matrix in which each line corresponds to a training example (out of `N` training instances) and each row corresponds to an individual feature, in this case the presence/absence of a particular subword in the sentence. In other words, there should be a total of `V` features, where `V` is the total size of the vocabulary for our BPE tokenizer. Also create a vector of length `N` with a value of `1` if the sentence was marked as Nynorsk, and 0 if is was marked as Bokmål. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word occurrences in corpus ndt_train_class.txt...Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532410d09b2c4fcf82c14944dd13556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"ndt_train_class.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    ndt_train_class = file.read()\n",
    "\n",
    "ndt_training_class = BPETokenizer2(train_corpus_file = 'ndt_train_class.txt', vocab_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I defined a function to create the design matrix. The function takes as input the text, the method and the vocabulary to use.\n",
    "\n",
    "def design_matrix_creation(a_text, a_method, vocabulary):\n",
    "    vocab_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    a_text_split = re.split(r\"\\n\", a_text)\n",
    "    FMatrix = np.zeros((len(a_text_split), len(vocabulary)))\n",
    "    f = np.zeros(len(a_text_split))\n",
    "\n",
    "    for i, sentence in enumerate(a_text_split[:-1]):\n",
    "        t = re.split(r\"\\t\", sentence)\n",
    "        tokenization = list(a_method(t[0], show_progress_bar=False))\n",
    "\n",
    "        for token in tokenization:\n",
    "            if token in vocab_index:\n",
    "                position = vocab_index[token]\n",
    "                FMatrix[i][position] = 1\n",
    "\n",
    "        if t[1] == 'nno':\n",
    "            f[i] = 1\n",
    "    return FMatrix, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ndt_training_class.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The design matrix is:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " The vector is:\n",
      " [1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "F_train_Matrix, f_train = design_matrix_creation(ndt_train_class, ndt_training_class, vocabulary)\n",
    "print(f'The design matrix is:\\n{F_train_Matrix}\\n\\n', f'The vector is:\\n {f_train}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.2__ (2 points): Use the data matrix you have just filled to train a logistic regression model (see the documentation on [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for more details). We recommend to use the `liblinear` solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "model.fit(F_train_Matrix, f_train)\n",
    "\n",
    "train_predictions = model.predict(F_train_Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.3__ (1 point): Now apply the learned logistic regression model to the test set in `ndt_test_class.txt`, and evaluate its performance in terms of accuracy, recall and precision (you can use the functionalities in `sklearn.metrics` to compute those easily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd68c32e42b48ab9d8ed6e11f1d1699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/608473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"ndt_test_class.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    ndt_test_class = file.read()\n",
    "\n",
    "ndt_testing_class = list(ndt_training_class(ndt_test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The design matrix is:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " The vector is:\n",
      " [1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "F_test_Matrix, f_test = design_matrix_creation(ndt_test_class, ndt_training_class, vocabulary)\n",
    "print(f'The design matrix is:\\n{F_test_Matrix}\\n\\n', f'The vector is:\\n {f_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(F_test_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9745\n",
      "Recall: 0.9627\n",
      "Precision: 0.9855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "accuracy = accuracy_score(f_test, test_predictions)\n",
    "recall = recall_score(f_test, test_predictions)\n",
    "precision = precision_score(f_test, test_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 3.4__ (2 points): Inspect the weights learned by your logistic regression model (in `coef_`) and find the 5 subwords that contribute _the most_ to the classification of the sentence in Nynorsk. Also find the 5 subwords that contribute the most to the classification of the sentence in Bokmål. Do those weights make sense, according to what you know about Bokmål and Nynorsk ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 subwords for Nynorsk:\n",
      "Subword: ikkje , Weight: 4.6689\n",
      "Subword: frå , Weight: 4.2924\n",
      "Subword: eit , Weight: 4.0350\n",
      "Subword: dei , Weight: 3.6508\n",
      "Subword: ein , Weight: 3.3407\n",
      "\n",
      "Top 5 subwords for Bokmål:\n",
      "Subword: ble , Weight: -2.7519\n",
      "Subword: ikke , Weight: -3.1778\n",
      "Subword: fra , Weight: -3.3179\n",
      "Subword: sier , Weight: -3.3302\n",
      "Subword: jeg , Weight: -3.3816\n"
     ]
    }
   ],
   "source": [
    "weights = model.coef_[0]\n",
    "\n",
    "weight_subword_map = {subword: weight for subword, weight in zip(vocabulary, weights)}\n",
    "\n",
    "top_nynorsk_subwords = sorted(weight_subword_map.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_bokmaal_subwords = sorted(weight_subword_map.items(), key=lambda x: x[1], reverse=True)[-5:]\n",
    "\n",
    "print(\"Top 5 subwords for Nynorsk:\")\n",
    "for subword, weight in top_nynorsk_subwords:\n",
    "    print(f\"Subword: {subword}, Weight: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 subwords for Bokmål:\")\n",
    "for subword, weight in top_bokmaal_subwords:\n",
    "    print(f\"Subword: {subword}, Weight: {weight:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
